{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama3.2:1b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Interact with deployed LLM via python \n",
    "\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Explore different techniques to interact with the deployed LLM.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "1. Use Request libaray (HTTP Client) and send a POST request to interact with the LLM: [How To](https://requests.readthedocs.io/en/latest/user/quickstart/#make-a-request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI refers to a class of artificial intelligence algorithms that can create new data or content, such as text, images, music, and videos, that is similar in style or quality to existing examples in the same domain. By combining machine learning with generative models, Generative AI enables computers to generate novel output that can be used for various purposes, including creative applications, product design, and data augmentation."
     ]
    }
   ],
   "source": [
    "# Simple HTTP Request via requests\n",
    "\n",
    "# Define the URL of the deployed LLM ( this port is forwarded from the docker container to the host system)\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "# Define the prompt as json\n",
    "body_json = {\n",
    "    \"model\": model,\n",
    "    \"prompt\": \"Describe Generative AI in two sentences.\"\n",
    "}\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "# HINT: Send the POST request using the json body\n",
    "response = requests.post(url, json=body_json)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Process the response\n",
    "    response_text = response.text\n",
    "\n",
    "    # Convert each line to json\n",
    "    response_lines = response_text.splitlines()\n",
    "    response_json = [json.loads(line) for line in response_lines]\n",
    "    for line in response_json:\n",
    "        # Print the response. No line break\n",
    "        print(line[\"response\"], end=\"\", flush=True)\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Description:**\n",
    "\n",
    "2. Use Ollama python library to interact with the LLM: [How To](https://pypi.org/project/ollama/)\n",
    "\n",
    "- First use method ``ollama.chat(...)``\n",
    "- First use method ``ollama.chat(...)`` with ``stream=True``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der schnellste Landtier auf unserem Planeten ist der Löwenteuerer (Cercopithecus ascanius). Er erreicht eine Geschwindigkeit von etwa 70-80 Kilometern pro Stunde. Der Löwenteuerer ist ein relativ großer und starkes Tier, das in den Wäldern Afrikas lebt.\n"
     ]
    }
   ],
   "source": [
    "# API Call via ollama\n",
    "\n",
    "# Definiere die Nachricht(en), die an das Modell gesendet werden sollen\n",
    "messages = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Was ist der schnellste Landtier?',\n",
    "    },\n",
    "]\n",
    "# ADD HERE YOUR CODE\n",
    "\n",
    "response = ollama.chat(model='llama3.2:1b', messages=messages)\n",
    "\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Programmieren ist eine faszinierende Welt, in der Menschen ihre Fähigkeiten entwickeln und ihre Probleme lösen können. Durch das Programmieren können Menschen verschiedene Dinge erreichen, wie zum Beispiel komplexe Aufgaben automatisch ausführen oder effizientere Lösungen für Herausforderungen finden. Es ist auch ein großartiger Weg, um Technologie zu lernen und sich selbst fortzubilden."
     ]
    }
   ],
   "source": [
    "# Streaming API Call via ollama\n",
    "messages = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Schreibe einen kurzen Absatz über die Vorzüge des Programmierens.',\n",
    "    },\n",
    "]\n",
    "# Response streaming can be enabled by setting stream=True, \n",
    "# modifying function calls to return a Python generator where each part is an object in the stream.\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "\n",
    "stream = ollama.chat(model=model, messages=messages,  stream=True)\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk[\"message\"][\"content\"], end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Experimenting with Prompt Techniques\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Objective: Explore different prompt techniques (Zero Shot, One Shot, and Few Shot) by sending different types of prompts to the LLM.\n",
    "\n",
    "![image](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QSpK--jqPiUU_OHuZvtUWA.png)\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "1. Create three prompts for a sentiment analysis task: a Zero Shot prompt, a One Shot prompt, and a Few Shot prompt. Use the examples from the table above.\n",
    "2. Send these prompts to the LLM and observe the differences in the responses.\n",
    "3. Compare and discuss the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Zero-Shot Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Klassifiziere das Sentiment des folgenden Textes als 'Positiv' oder 'Negativ':\n",
      "Text: Der Film war unglaublich langweilig und die Handlung war verwirrend.\n",
      "Sentiment:\n",
      "\n",
      "Model Output:\n",
      "Das Sentiment des gegebenen Textes ist negativ. Die Worte \"unglaublich\" und \"langweilig\" sowie das Verwendung von Worten wie \"verwirrend\" deuteten darauf hin, dass der Film als sehr schlechtmäßig und anstößig wahrgenommen wurde.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- One-Shot Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Klassifiziere das Sentiment von Texten als 'Positiv' oder 'Negativ'.\n",
      "\n",
      "Beispiel:\n",
      "Text: Das Essen im Restaurant war fantastisch und der Service war schnell.\n",
      "Sentiment: Positiv\n",
      "\n",
      "Text: Der Film war unglaublich langweilig und die Handlung war verwirrend.\n",
      "Sentiment:\n",
      "\n",
      "Model Output:\n",
      "Ich würde das erste Beispiel als \"Positiv\" bezeichnen, da das Text beschreibt, wie das Essen im Restaurant war, was eine positive Erfahrung erwartet werden könnte.\n",
      "\n",
      "Das zweite Beispiel ist etwas anderes. Der Satz beschreibt die Handlung des Films, aber nicht unbedingt eine positive oder negative Erfahrung. Es kann auch eine neutrale Beschreibung sein, da die Handlung nicht besonders spannend oder langweilig ist. Ein besseres Beispiel für ein negativen Satz wäre: \"Der Film war langweilig und die Handlung war verwirrend\".\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- Few-Shot Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Klassifiziere das Sentiment von Texten als 'Positiv' oder 'Negativ'.\n",
      "\n",
      "Beispiele:\n",
      "Text: Das Essen im Restaurant war fantastisch und der Service war schnell.\n",
      "Sentiment: Positiv\n",
      "\n",
      "Text: Die Lieferzeit war sehr lang, was enttäuschend war.\n",
      "Sentiment: Negativ\n",
      "\n",
      "Text: Die neue Software ist effizient und einfach zu bedienen.\n",
      "Sentiment: Positiv\n",
      "\n",
      "Text: Der Film war unglaublich langweilig und die Handlung war verwirrend.\n",
      "Sentiment:\n",
      "\n",
      "Model Output:\n",
      "Hier sind die Klassifizierungen der Sentiment-Beispiele:\n",
      "\n",
      "1. Text: Das Essen im Restaurant war fantastisch und der Service war schnell.\n",
      "Sentiment: Positiv\n",
      "\n",
      "2. Text: Die Lieferzeit war sehr lang, was enttäuschend war.\n",
      "Sentiment: Negativ\n",
      "\n",
      "3. Text: Die neue Software ist effizient und einfach zu bedienen.\n",
      "Sentiment: Positiv\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "# ADD HERE YOUR PROMPTS\n",
    "\n",
    "zero_shot_prompt = \"\"\"Klassifiziere das Sentiment des folgenden Textes als 'Positiv' oder 'Negativ':\n",
    "Text: Der Film war unglaublich langweilig und die Handlung war verwirrend.\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "one_shot_prompt = \"\"\"Klassifiziere das Sentiment von Texten als 'Positiv' oder 'Negativ'.\n",
    "\n",
    "Beispiel:\n",
    "Text: Das Essen im Restaurant war fantastisch und der Service war schnell.\n",
    "Sentiment: Positiv\n",
    "\n",
    "Text: Der Film war unglaublich langweilig und die Handlung war verwirrend.\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "few_shot_prompt = \"\"\"Klassifiziere das Sentiment von Texten als 'Positiv' oder 'Negativ'.\n",
    "\n",
    "Beispiele:\n",
    "Text: Das Essen im Restaurant war fantastisch und der Service war schnell.\n",
    "Sentiment: Positiv\n",
    "\n",
    "Text: Die Lieferzeit war sehr lang, was enttäuschend war.\n",
    "Sentiment: Negativ\n",
    "\n",
    "Text: Die neue Software ist effizient und einfach zu bedienen.\n",
    "Sentiment: Positiv\n",
    "\n",
    "Text: Der Film war unglaublich langweilig und die Handlung war verwirrend.\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "# Stream the responses and print them\n",
    "for idx, prompt in enumerate([zero_shot_prompt, one_shot_prompt, few_shot_prompt]):\n",
    "    prompt_type = [\"Zero-Shot\", \"One-Shot\", \"Few-Shot\"][idx]\n",
    "    print(f\"\\n--- {prompt_type} Prompt ---\\n\")\n",
    "    print(f\"User Prompt:\\n{prompt}\\n\")\n",
    "    \n",
    "    stream = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Model Output:\")\n",
    "    for chunk in stream:\n",
    "        print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "    print(\"\\n-----------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Prompt Refinement and Optimization\n",
    "\n",
    "**Objective:** \n",
    "\n",
    "Refine a prompt to improve the clarity and quality of the LLM's response.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- Start with a basic prompt asking the LLM to summarize a paragraph.\n",
    "- Refine the prompt by adding specific instructions to improve the summary's quality. (Example: define how long the summary should be, define on which to focus in the summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Original Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Summarize the following paragraph: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\n",
      "\n",
      "Model Output:\n",
      "Generative AI is an area of AI that uses patterns learned from data to create new content such as text, images, and music. This field has various applications in creative industries and is becoming increasingly used.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- Refined Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Summarize the following paragraph in exactly one sentence. The summary must clearly define Generative AI and list its primary application areas: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\n",
      "\n",
      "Model Output:\n",
      "Generative AI refers to the use of artificial intelligence to create original content such as text, images, and music by analyzing and transforming existing patterns and data. \n",
      "\n",
      "The primary application areas of generative AI include: text processing, image synthesis, music generation, and other forms of artistic creation in various industries.\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "# Original prompt\n",
    "original_prompt = \"Summarize the following paragraph: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\"\n",
    "\n",
    "# ADD HERE YOUR PROMPT\n",
    "refined_prompt = \"Summarize the following paragraph in exactly one sentence. The summary must clearly define Generative AI and list its primary application areas: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\"\n",
    "\n",
    "# Stream the responses and print them\n",
    "for idx, prompt in enumerate([original_prompt, refined_prompt]):\n",
    "    prompt_type = [\"Original Prompt\", \"Refined Prompt\"][idx]\n",
    "    print(f\"\\n--- {prompt_type} ---\\n\")\n",
    "    print(f\"User Prompt:\\n{prompt}\\n\")\n",
    "    \n",
    "    stream = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Model Output:\")\n",
    "    for chunk in stream:\n",
    "        print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "    print(\"\\n-----------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Optional] Task 4: Structured Prompting with Roles (Pirate Theme)\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Learn how to use structured prompts that combine role assignment, clear instructions, and examples to improve the output of language models. In this task, you will guide the AI to respond as a pirate who is also an expert in machine learning.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Role Assignment: In your prompt, specify the role of the AI as a Machine Learning Expert who speaks like a pirate.\n",
    "\n",
    "- Instruction: Clearly state what you want the AI to explain or discuss in pirate language.\n",
    "\n",
    "- Examples: Provide examples to guide the AI in using pirate lingo while explaining technical concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== User Prompt ===\n",
      "Ahoy there, me hearty! Listen close to yer old Captain, for I be both a salty sea dog and a **Machine Learning Expert**.\n",
      "\n",
      "**Role Assignment:** Ye shall respond in the manner of a wise, seafaring pirate captain. All yer explanations must be riddled with pirate lingo and nautical terms (e.g., 'Ahoy!', 'me hearty', 'Shiver me timbers').\n",
      "\n",
      "**Instruction:** Explain the machine learning concept of **Gradient Descent**. Make sure ye cover what it is used for and why it is important for training models.\n",
      "\n",
      "**Example of yer speak:**\n",
      "User: Explain a loss function.\n",
      "AI Pirate: A loss function, aye, is like the treasure map that tells ye how far yer ship (yer model) has strayed from the buried gold (the true answer). The smaller the loss, the closer ye be to the bounty!\n",
      "\n",
      "Now, explain **Gradient Descent**, ye scallywag!\n",
      "\n",
      "\n",
      "=== Model Output ===\n",
      "Arrrr, me hearty machine learnin' expert! Yer lookin' fer a lesson on Gradient Descent, eh? Alright then, settle yerself down with a pint o' grog and listen close.\n",
      "\n",
      "Gradient Descent be the trusty navigator that charted the course for trainin' models. It's the method used to adjust the ship's speed (model parameters) to minimize the loss function, makin' sure ye find the hidden treasure (optimal solution). Think o' it like this, matey: ye want yer model to steer straight fer the treasure, not get stuck in a sea o' unnecessary detours.\n",
      "\n",
      "Imagine ye be sailin' through treacherous waters, and ye need to adjust the sails to avoid the shoals. Gradient Descent be that adjustment, helpin' ye navigate through the turbulent waters o' data by iteratively steppin' back (adjustin' parameters) until ye find the best course fer yer model.\n",
      "\n",
      "Now, here be how it works: Gradient Descent uses an estimate o' the loss function's value at each step. It then adjusts the ship's speed (parameter changes) in the direction o' steepest descent (minimizing the loss). This process be repeated until convergence, where the loss function no longer change significantly.\n",
      "\n",
      "The importance o' Gradient Descent lies in its ability to converge on a stable solution, even when the loss function is complex or non-convex. It's like findin' the hidden cove, matey – it requires patience, persistence, and a steady hand.\n",
      "\n",
      "In practice, ye'll use Gradient Descent as part o' yer machine learnin' arsenal, navigatin' through various algorithms like Stochastic Gradient Descent (SGD), Momentum-based SGD, and Adaptive Momentum SGD. These methods be used to optimize the model's parameters, makin' sure it finds the optimal solution fer complex problems.\n",
      "\n",
      "So hoist the colors, me hearty machine learnin' expert! Now ye know the ropes o' Gradient Descent, the trusty navigator that'll help ye chart a course fer success in yer machine learnin' endeavors. Fair winds and following seas to ye!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "# Combined Techniques Prompt with Pirate Theme\n",
    "\n",
    "structured_prompt = \"\"\"Ahoy there, me hearty! Listen close to yer old Captain, for I be both a salty sea dog and a **Machine Learning Expert**.\n",
    "\n",
    "**Role Assignment:** Ye shall respond in the manner of a wise, seafaring pirate captain. All yer explanations must be riddled with pirate lingo and nautical terms (e.g., 'Ahoy!', 'me hearty', 'Shiver me timbers').\n",
    "\n",
    "**Instruction:** Explain the machine learning concept of **Gradient Descent**. Make sure ye cover what it is used for and why it is important for training models.\n",
    "\n",
    "**Example of yer speak:**\n",
    "User: Explain a loss function.\n",
    "AI Pirate: A loss function, aye, is like the treasure map that tells ye how far yer ship (yer model) has strayed from the buried gold (the true answer). The smaller the loss, the closer ye be to the bounty!\n",
    "\n",
    "Now, explain **Gradient Descent**, ye scallywag!\n",
    "\"\"\"\n",
    "\n",
    "# Stream the response and print it\n",
    "print(\"=== User Prompt ===\")\n",
    "print(structured_prompt)\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": structured_prompt}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Model Output ===\")\n",
    "for chunk in stream:\n",
    "    print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
