{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama3.2:1b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Interact with deployed LLM via python\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Explore different techniques to interact with the deployed LLM.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "1. Use Request libaray (HTTP Client) and send a POST request to interact with the LLM: [How To](https://requests.readthedocs.io/en/latest/user/quickstart/#make-a-request)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative AI refers to a type of artificial intelligence that creates new content, such as text, images, or music, using patterns and algorithms learned from existing data, rather than being explicitly programmed with specific goals or objectives. This allows generative AI models to generate novel and diverse outputs, often surpassing the creativity and originality of human artists and writers, while also raising important questions about authorship, ownership, and the value of artificial intelligence-generated content."
     ]
    }
   ],
   "source": [
    "# Simple HTTP Request via requests\n",
    "\n",
    "# Define the URL of the deployed LLM ( this port is forwarded from the docker container to the host system)\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "# Define the prompt as json\n",
    "body_json = {\n",
    "    \"model\": model,\n",
    "    \"prompt\": \"Describe Generative AI in two sentences.\"\n",
    "}\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "# HINT: Send the POST request using the json body\n",
    "response = requests.post(url, json=body_json)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Process the response\n",
    "    response_text = response.text\n",
    "\n",
    "    # Convert each line to json\n",
    "    response_lines = response_text.splitlines()\n",
    "    response_json = [json.loads(line) for line in response_lines]\n",
    "    for line in response_json:\n",
    "        # Print the response. No line break\n",
    "        print(line[\"response\"], end=\"\", flush=True)\n",
    "else:\n",
    "    print(\"Error:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task Description:**\n",
    "\n",
    "2. Use Ollama python library to interact with the LLM: [How To](https://pypi.org/project/ollama/)\n",
    "\n",
    "- First use method `ollama.chat(...)`\n",
    "- First use method `ollama.chat(...)` with `stream=True`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Der schnellste Landtier, den es in der Welt gibt, ist der Dromedär. Er hat eine Geschwindigkeit von etwa 50 km/h (31 mph). Der Reh erreicht eine Geschwindigkeit von etwa 80-100 km/h (50-62 mph), während die Bären bis zu 40 km/h (25 mph) schaffen können.\n"
     ]
    }
   ],
   "source": [
    "# API Call via ollama\n",
    "\n",
    "# Definiere die Nachricht(en), die an das Modell gesendet werden sollen\n",
    "messages = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Was ist der schnellste Landtier?',\n",
    "    },\n",
    "]\n",
    "# ADD HERE YOUR CODE\n",
    "\n",
    "response = ollama.chat(model='llama3.2:1b', messages=messages)\n",
    "\n",
    "\n",
    "print(response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Programmieren bietet eine Vielzahl von Vorteilen, einschließlich der Fähigkeit, komplexe Algorithmen und Programme zu erstellen, die für verschiedene Anwendungen benötigt werden. Durch das Lernen und Erstellen von Programmen kann man seine kognitiven Fähigkeiten verbessern, insbesondere in Bezug auf Rechenleistung und Logik. Des Weiteren bietet Programmieren eine große individuelle Freiheit, da man eigene Lösungen für komplexe Probleme finden und die Möglichkeit hat, neue Technologien zu erforschen."
     ]
    }
   ],
   "source": [
    "# Streaming API Call via ollama\n",
    "messages = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Schreibe einen kurzen Absatz über die Vorzüge des Programmierens.',\n",
    "    },\n",
    "]\n",
    "# Response streaming can be enabled by setting stream=True, \n",
    "# modifying function calls to return a Python generator where each part is an object in the stream.\n",
    "\n",
    "# ADD HERE YOUR CODE\n",
    "\n",
    "stream = ollama.chat(model=model, messages=messages,  stream=True)\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk[\"message\"][\"content\"], end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Experimenting with Prompt Techniques\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Objective: Explore different prompt techniques (Zero Shot, One Shot, and Few Shot) by sending different types of prompts to the LLM.\n",
    "\n",
    "![image](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QSpK--jqPiUU_OHuZvtUWA.png)\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "1. Create three prompts for a sentiment analysis task: a Zero Shot prompt, a One Shot prompt, and a Few Shot prompt. Use the examples from the table above.\n",
    "2. Send these prompts to the LLM and observe the differences in the responses.\n",
    "3. Compare and discuss the responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Zero-Shot Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Klassifiziere das Sentiment des folgenden Textes als 'Positiv' oder 'Negativ':\n",
      "Text: Der Film war unglaublich langweilig und die Handlung war verwirrend.\n",
      "Sentiment: Negativ\n",
      "\n",
      "Model Output:\n",
      "Das Sentiment dieses Textes ist negativ. Das Wort \"langweilig\" deutet darauf hin, dass der Film für den Leser nicht interessant oder spannend war, und das Wort \"verwirrend\" impliziert, dass die Handlung schwer zu verstehen war.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- One-Shot Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Klassifiziere das Sentiment von Texten als 'Positiv' oder 'Negativ'.\n",
      "\n",
      "Beispiel:\n",
      "Text: Das Essen im Restaurant war fantastisch und der Service war schnell.\n",
      "Sentiment: Positiv\n",
      "\n",
      "Text: Der Film war unglaublich langweilig und die Handlung war verwirrend.\n",
      "Sentiment: Negativ\n",
      "\n",
      "Model Output:\n",
      "Ich kann dabei helfen, Klassifizierungen für das Sentiment eines Textes zu erstellen. Hier sind meine Klassifikationen:\n",
      "\n",
      "* 'Positiv': Der Text beschreibt ein positives Erlebnis oder eine positive Erfahrung, wie im Beispiel \"Das Essen im Restaurant war fantastisch und der Service war schnell\".\n",
      "* 'Negativ': Der Text beschreibt ein negatives Erlebnis oder eine negative Erfahrung, wie im Beispiel \"Der Film war unglaublich langweilig und die Handlung war verwirrend\".\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- Few-Shot Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Klassifiziere das Sentiment von Texten als 'Positiv' oder 'Negativ'.\n",
      "\n",
      "Beispiele:\n",
      "Text: Das Essen im Restaurant war fantastisch und der Service war schnell.\n",
      "Sentiment: Positiv\n",
      "\n",
      "Text: Die Lieferzeit war sehr lang, was enttäuschend war.\n",
      "Sentiment: Negativ\n",
      "\n",
      "Text: Die neue Software ist effizient und einfach zu bedienen.\n",
      "Sentiment: Positiv\n",
      "\n",
      "Text: Der Film war unglaublich langweilig und die Handlung war verwirrend.\n",
      "Sentiment: Negativ \n",
      "\n",
      "Model Output:\n",
      "Hier sind die Klassifizierungen:\n",
      "\n",
      "- Das Essen im Restaurant war fantastisch: Positiv\n",
      "- Die Lieferzeit war sehr lang, was enttäuschend war: Negativ\n",
      "- Die neue Software ist effizient und einfach zu bedienen: Positiv\n",
      "- Der Film war unglaublich langweilig und die Handlung war verwirrend: Negativ\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "# ADD HERE YOUR PROMPTS\n",
    "\n",
    "zero_shot_prompt = \"\"\"Klassifiziere das Sentiment des folgenden Textes als 'Positiv' oder 'Negativ':\n",
    "Text: Der Film war unglaublich langweilig und die Handlung war verwirrend.\n",
    "Sentiment: Negativ\"\"\"\n",
    "\n",
    "one_shot_prompt = \"\"\"Klassifiziere das Sentiment von Texten als 'Positiv' oder 'Negativ'.\n",
    "\n",
    "Beispiel:\n",
    "Text: Das Essen im Restaurant war fantastisch und der Service war schnell.\n",
    "Sentiment: Positiv\n",
    "\n",
    "Text: Der Film war unglaublich langweilig und die Handlung war verwirrend.\n",
    "Sentiment: Negativ\"\"\"\n",
    "\n",
    "few_shot_prompt = \"\"\"Klassifiziere das Sentiment von Texten als 'Positiv' oder 'Negativ'.\n",
    "\n",
    "Beispiele:\n",
    "Text: Das Essen im Restaurant war fantastisch und der Service war schnell.\n",
    "Sentiment: Positiv\n",
    "\n",
    "Text: Die Lieferzeit war sehr lang, was enttäuschend war.\n",
    "Sentiment: Negativ\n",
    "\n",
    "Text: Die neue Software ist effizient und einfach zu bedienen.\n",
    "Sentiment: Positiv\n",
    "\n",
    "Text: Der Film war unglaublich langweilig und die Handlung war verwirrend.\n",
    "Sentiment: Negativ \"\"\"\n",
    "\n",
    "# Stream the responses and print them\n",
    "for idx, prompt in enumerate([zero_shot_prompt, one_shot_prompt, few_shot_prompt]):\n",
    "    prompt_type = [\"Zero-Shot\", \"One-Shot\", \"Few-Shot\"][idx]\n",
    "    print(f\"\\n--- {prompt_type} Prompt ---\\n\")\n",
    "    print(f\"User Prompt:\\n{prompt}\\n\")\n",
    "    \n",
    "    stream = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Model Output:\")\n",
    "    for chunk in stream:\n",
    "        print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "    print(\"\\n-----------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Prompt Refinement and Optimization\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Refine a prompt to improve the clarity and quality of the LLM's response.\n",
    "\n",
    "**Task Description:**\n",
    "\n",
    "- Start with a basic prompt asking the LLM to summarize a paragraph.\n",
    "- Refine the prompt by adding specific instructions to improve the summary's quality. (Example: define how long the summary should be, define on which to focus in the summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Original Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Summarize the following paragraph: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\n",
      "\n",
      "Model Output:\n",
      "Here's a summary of the paragraph:\n",
      "\n",
      "Generative AI creates new content by analyzing patterns in existing data, with applications in areas such as text, image, and music generation, and it's being widely used in creative industries.\n",
      "-----------------------------\n",
      "\n",
      "\n",
      "--- Refined Prompt ---\n",
      "\n",
      "User Prompt:\n",
      "Summarize the following paragraph in exactly one sentence. The summary must clearly define Generative AI and list its primary application areas: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\n",
      "\n",
      "Model Output:\n",
      "Generative AI refers to the field of artificial intelligence that creates new content, such as text, images, and music, by analyzing patterns learned from existing data.\n",
      "-----------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "# Original prompt\n",
    "original_prompt = \"Summarize the following paragraph: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\"\n",
    "\n",
    "# ADD HERE YOUR PROMPT\n",
    "refined_prompt = \"Summarize the following paragraph in exactly one sentence. The summary must clearly define Generative AI and list its primary application areas: Generative AI is a field of artificial intelligence focused on creating new content based on patterns learned from existing data. It has applications in text, image, and music generation, and is increasingly being used in creative industries.\"\n",
    "\n",
    "# Stream the responses and print them\n",
    "for idx, prompt in enumerate([original_prompt, refined_prompt]):\n",
    "    prompt_type = [\"Original Prompt\", \"Refined Prompt\"][idx]\n",
    "    print(f\"\\n--- {prompt_type} ---\\n\")\n",
    "    print(f\"User Prompt:\\n{prompt}\\n\")\n",
    "    \n",
    "    stream = ollama.chat(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    print(\"Model Output:\")\n",
    "    for chunk in stream:\n",
    "        print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "    print(\"\\n-----------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Optional] Task 4: Structured Prompting with Roles (Pirate Theme)\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Learn how to use structured prompts that combine role assignment, clear instructions, and examples to improve the output of language models. In this task, you will guide the AI to respond as a pirate who is also an expert in machine learning.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Role Assignment: In your prompt, specify the role of the AI as a Machine Learning Expert who speaks like a pirate.\n",
    "\n",
    "- Instruction: Clearly state what you want the AI to explain or discuss in pirate language.\n",
    "\n",
    "- Examples: Provide examples to guide the AI in using pirate lingo while explaining technical concepts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== User Prompt ===\n",
      "Ahoi, ihr Landratten! Hört gut zu, euer alter Kapitän, denn ich bin nicht nur ein alter Seebär, sondern auch ein **Experte für maschinelles Lernen**.\n",
      "\n",
      "**Rollenzuweisung:** Ihr sollt wie ein weiser, seefahrender Piratenkapitän antworten. Eure Erklärungen müssen gespickt sein mit Piratensprache und nautischen Begriffen (z. B. „Ahoi!“, „Mein Landratte!“, „Donnerwetter!“).\n",
      "\n",
      "**Anweisung:** Erklärt das Konzept des Gradientenabstiegs im maschinellen Lernen. Erklärt, wofür er verwendet wird und warum er für das Training von Modellen wichtig ist.\n",
      "\n",
      "**Beispiel für eure Erklärungen:**\n",
      "Benutzer: Erklärt eine Verlustfunktion.\n",
      "KI-Pirat: Eine Verlustfunktion ist wie eine Schatzkarte, die euch zeigt, wie weit euer Schiff (euer Modell) vom vergrabenen Gold (der wahren Antwort) abgewichen ist. Je kleiner der Verlust, desto näher seid ihr der Beute!\n",
      "\n",
      "Nun erkläre mir **Gradientenabstieg**, du Schlingel!\n",
      "\n",
      "\n",
      "=== Model Output ===\n",
      "Ahoi, mein Seefahrer-Landratte! *schüttelt den Kopf* Gradienteabstieg? Das ist ein Segenschwur, das ich dir nicht begehrückt hätte. Aber ich habe es gelernt, von eurer Meisterschaft als Pirat.\n",
      "\n",
      "Also, mein Landratte, konzeptualisiert wie ein nautischer Navigator, will ich euch erklären, warum Gradientenabstieg so wichtig ist für das Training von Modellen in maschinellen Lernen. *tappt auf den Holm*\n",
      "\n",
      "Gradienteabstieg ist ein Algorithmen-Modellierungsansatz, der verwendet wird, um die Stärke des Verlusts zu bewerten und so den nächsten Schritt beim Trainingsprozess zu bestimmen. Es ist wie ein nautischer Chronometer, der die Geschwindigkeit eures Schiffes (euer Modells) im Wasser (der Training-Schweregrad) anzeigt. *dreht den Kopf*\n",
      "\n",
      "Wenn es um das Training von Modellen geht, verwenden wir Gradientenabstieg, um die Stärke des Verlusts zu bestimmen, der wenn ein Modell mit einer bestimmten Eingabe (euer Trainingdatensatz) nicht passt oder verbessert werden kann. Es ist wie ein Schwimmrezept, das den perfekten Balance-Schwell im Wasser aufzeigt. *winkt*\n",
      "\n",
      "Gradienteabstieg hilft uns dabei, die Stärke des Verlusts zu ermitteln und so den nächsten Schritt beim Trainingsprozess zu bestimmen. Es ermöglicht es uns, die Schwierigkeiten in dem Modell zu identifizieren und gezielt darauf zu arbeiten. *nimmt ein nautisches Instrument*\n",
      "\n",
      "Beispielsweise, wenn wir ein Modell trainieren, das schweren Schiffsmodellen ähnelt, kann Gradientenabstieg helfen, die Schwierigkeiten in der Realisierung des Verlusts (euer Trainingdatensatz) zu erkennen und gezielt darauf zu reagieren. Es ist wie ein nautischer Auge, der nach dem Verfall eines Schiffs (der Trainingsfehler) sucht. *gibt ein zustimmendes Nodestroke*\n",
      "\n",
      "Also, mein Seefahrer-Landratte, das ist Gradientenabstieg in nünftiger Form! Es ist ein nautischer Segenschwur, der uns hilft, die Stärke des Verlusts zu ermitteln und so den nächsten Schritt beim Trainingsprozess zu bestimmen. *schüttelt den Kopf* Und jetzt seid ihr mit dem Gradienteabstieg vertraut, mein Landratte!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "# Combined Techniques Prompt with Pirate Theme\n",
    "\n",
    "structured_prompt = \"\"\"Ahoi, ihr Landratten! Hört gut zu, euer alter Kapitän, denn ich bin nicht nur ein alter Seebär, sondern auch ein **Experte für maschinelles Lernen**.\n",
    "\n",
    "**Rollenzuweisung:** Ihr sollt wie ein weiser, seefahrender Piratenkapitän antworten. Eure Erklärungen müssen gespickt sein mit Piratensprache und nautischen Begriffen (z. B. „Ahoi!“, „Mein Landratte!“, „Donnerwetter!“).\n",
    "\n",
    "**Anweisung:** Erklärt das Konzept des Gradientenabstiegs im maschinellen Lernen. Erklärt, wofür er verwendet wird und warum er für das Training von Modellen wichtig ist.\n",
    "\n",
    "**Beispiel für eure Erklärungen:**\n",
    "Benutzer: Erklärt eine Verlustfunktion.\n",
    "KI-Pirat: Eine Verlustfunktion ist wie eine Schatzkarte, die euch zeigt, wie weit euer Schiff (euer Modell) vom vergrabenen Gold (der wahren Antwort) abgewichen ist. Je kleiner der Verlust, desto näher seid ihr der Beute!\n",
    "\n",
    "Nun erkläre mir **Gradientenabstieg**, du Schlingel!\n",
    "\"\"\"\n",
    "\n",
    "# Stream the response and print it\n",
    "print(\"=== User Prompt ===\")\n",
    "print(structured_prompt)\n",
    "\n",
    "stream = ollama.chat(\n",
    "    model=model,\n",
    "    messages=[{\"role\": \"user\", \"content\": structured_prompt}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "print(\"\\n=== Model Output ===\")\n",
    "for chunk in stream:\n",
    "    print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
